# Ê†áÈ¢òÂíåÊëòË¶Å

**Èöè‰æøÊâæÁöÑÔºå‰∏çÁü•ÈÅìË¥®ÈáèÊÄé‰πàÊ†∑**

## 1. A Bio-Inspired Multi-Exposure Fusion Framework for Low-light Image Enhancement

Low-light images are not conducive to human observation and computer vision algorithms due to their low visibility. Although many image enhancement techniques have been proposed to solve this problem, existing methods inevitably introduce contrast under- and over-enhancement. Inspired by human visual system, we design a multi-exposure fusion framework for low-light image enhancement. Based on the framework, we propose a dual-exposure fusion algorithm to provide an accurate contrast and lightness enhancement. Specifically, we first design the weight matrix for image fusion using illumination estimation techniques. Then we introduce our camera response model to synthesize multi-exposure images. Next, we find the best exposure ratio so that the synthetic image is well-exposed in the regions where the original image is under-exposed. Finally, the enhanced result is obtained by fusing the input image and the synthetic image according to the weight matrix. Experiments show that our method can obtain results with less contrast and lightness distortion compared to that of several state-of-the-art methods.

## 2. A Retinal Mechanism Inspired Color Constancy Model

In this paper, we propose a novel model for the computational color constancy, inspired by the amazing ability of the human vision system (HVS) to perceive the color of objects largely constant as the light source color changes. The proposed model imitates the color processing mechanisms in the specific level of the retina, the first stage of the HVS, from the adaptation emerging in the layers of cone photoreceptors and horizontal cells (HCs) to the color-opponent mechanism and disinhibition effect of the non-classical receptive field in the layer of retinal ganglion cells (RGCs). In particular, HC modulation provides a global color correction with cone-specific lateral gain control, and the following RGCs refine the processing with iterative adaptation until all the three opponent channels reach their stable states (i.e., obtain stable outputs). Instead of explicitly estimating the scene illuminant(s), such as most existing algorithms, our model directly removes the effect of scene illuminant. Evaluations on four commonly used color constancy data sets show that the proposed model produces competitive results in comparison with the state-of-the-art methods for the scenes under either single or multiple illuminants. The results indicate that single opponency, especially the disinhibitory effect emerging in the receptive field√¢‚Ç¨‚Ñ¢s subunit-structured surround of RGCs, plays an important role in removing scene illuminant(s) by inherently distinguishing the spatial structures of surfaces from extensive illuminant(s).

## 3. Application-Oriented Retinal Image Models for Computer Vision

Energy and storage restrictions are relevant variables that software applications should be concerned about when running in low-power environments. In particular, computer vision (CV) applications exemplify well that concern, since conventional uniform image sensors typically capture large amounts of data to be further handled by the appropriate CV algorithms. Moreover, much of the acquired data are often redundant and outside of the application√¢‚Ç¨‚Ñ¢s interest, which leads to unnecessary processing and energy spending. In the literature, techniques for sensing and re-sampling images in non-uniform fashions have emerged to cope with these problems. In this study, we propose Application-Oriented Retinal Image Models that define a space-variant configuration of uniform images and contemplate requirements of energy consumption and storage footprints for CV applications. We hypothesize that our models might decrease energy consumption in CV tasks.
Moreover, we show how to create the models and validate their use in a face detection/recognition application, evidencing the compromise between storage, energy, and accuracy.

## 4. Automatic Image Enhancement from a Mobile Synthetic Vision System

The work discusses basic image contrasting algorithms and noise compensation methods, an algorithm for estimating image quality based on an integral quality indicator, as well as approaches for estimating noise values in images. The results of contrasting algorithms work (with a numerical estimation) and the most prominent image filtering methods are presented. A description is given for an automatic image enhancement algorithm from a mobile synthetic vision system based on a choice of contrasting algorithms using an integral quality indicator, and a space-time filter using a pyramidal version of Lucas-Kanade optical flow algorithm is proposed.

## 5. Bio-inspired color image enhancement

Capturing and rendering an image that fullls the observer's expectations is a dicult task. This is due to the fact that the signal reaching the eye is processed by a complex mechanism before forming a percept, whereas a capturing device only retains the physical value of light intensities. It is especially dicult to render complex scenes with highly varying luminances. For example, a picture taken inside a room where objects are visible through the windows will not be rendered correctly by a global technique. Either details in the dim room will be hidden in shadow or the objects viewed through the window will be too bright. The image has to be treated locally to resemble more closely to what the observer remembers. The purpose of this work is to develop a technique for rendering images based on human local adaptation.
We take inspiration from a model of color vision called Retinex. This model determines the perceived color given spatial relationships of the captured signals. Retinex has been used as a computational model for image rendering. In this article, we propose a new solution inspired by Retinex that is based on a single lter applied to the luminance channel. All parameters are image-dependent so that the process requires no parameter tuning.
That makes the method more  exible than other existing ones. The presented results show that our method suitably enhances high dynamic range images.

## 6. Bio-inspired computer vision: Towards a synergistic approach of artificial and biological vision

Studies in biological vision have always been a great source of inspiration for design of computer vision algorithms. In the past, several successful methods were designed with varying degrees of correspon- dence with biological vision studies, ranging from purely functional inspiration to methods that utilise models that were primarily developed for explaining biological observations. Even though it seems well recognised that computational models of biological vision can help in design of computer vision algo- rithms, it is a non-trivial exercise for a computer vision researcher to mine relevant information from biological vision literature as very few studies in biology are organised at a task level. In this paper we aim to bridge this gap by providing a computer vision task centric presentation of models primarily orig- inating in biological vision studies. Not only do we revisit some of the main features of biological vision and discuss the foundations of existing computational studies modelling biological vision, but also we consider three classical computer vision tasks from a biological per spective: image sensing, segmentation and optical flow. Using this task-centric approach, we discuss well-known biological functional princi- ples and compare them with approaches taken by computer vision. Based on this comparative analysis of computer and biological vision, we present some recent models in biological vision and highlight a few models that we think are promising for future investigations in computer vision. To this extent, this pa- per provides new insights and a starting point for investigators interested in the design of biology-based computer vision algorithms and pave a way for much needed interaction between the two communities leading to the development of synergistic models of artificial and biological vision.

## 7. Collaborative Image Triage with Humans and Computer Vision

As the technology for acquiring and storing images becomes more prevalent, we are faced with a growing need to sort and label these images. At this time, computer vision algorithms cannot parse abstract concepts from images like a human. As a result, there may be performance gains possible from the integration of human analysts with computer vision agents. We present an image triage system which facilitates the collaboration of heterogeneous agents through a novel unsupervised metalearning technique. The system iteratively allocates images for binary classification among heterogeneous agents according to the Generalized Assignment Problem (GAP) and combines the classification results using the Spectral Meta-Learner (SML). In simulation, we demonstrate that the proposed system achieves significant speed-up over a naive parallel assignment strategy without sacrificing accuracy.

## 8. Common Principles of Image Acquisition Systems and Biological Vision

In this paper, we argue that biological vision and electronic image acquisition share common principles despite their vastly different implementations. These shared principles are based on the need to acquire a common set of input stimuli as well as the need to generalize from the acquired images. Two related principles are discussed in detail, namely, multiple parallel image representations and the use of dedicated local memory in various stages of acquisition and processing. We review relevant literature in visual neuroscience and image systems engineering to support our argument. Particularly, the paper discusses multiple capture image acquisition, with applications such as dynamic range, field-of-view, or depth-of-field extension. Finally, as an example, a novel multiplecapturesingle-image complementary metal√¢‚Ç¨‚Äúoxide√¢‚Ç¨‚Äúsemiconductor sensor is presented. This sensor has been developed at Stanford University and it illustrates the principles that are shared among biological vision and image acquisition.

## 9. Evaluation of Image Enhancement Techniques for Vision-Based Navigation under Low Illumination

Cameras are valuable sensors for robotics perception tasks. Among these perception tasks aremotion estimation, localization, and object detection. Cameras are attractive sensors because they are passive and relatively cheap and can provide rich information.
However, being passive sensors, they rely on external illumination from the environment which means that their performance degrades in low-light conditions. In this paper, we present and investigate four methods to enhance images under challenging night conditions. The findings are relevant to a wide range of feature-based vision systems, such as tracking for augmented reality, image registration, localization, and mapping, as well as deep learning-based object detectors. As autonomous mobile robots are expected to operate under low-illumination conditions at night, evaluation is based on state-of-the-art systems for motion estimation, localization, and object detection.

## 10. Human Visual System-Based Image Enhancement and Logarithmic Contrast Measure

Varying scene illumination poses many challenging problems for machine vision systems. One such issue is developing global enhancement methods that work effectively across the varying illumination. In this paper, we introduce two novel image enhancement algorithms: edge-preserving contrast enhancement, which is able to better preserve edge details while enhancing contrast in images with varying illumination, and a novel multihistogram equalization method which utilizes the human visual system (HVS) to segment the image, allowing a fast and efficient correction of nonuniform illumination. We then extend this HVS-based multihistogram equalization approach to create a general enhancement method that can utilize any combination of enhancement algorithms for an improved performance. Additionally, we propose new quantitativemeasures of image enhancement, called the logarithmicMichelson contrast measure (AME) and the logarithmic AME by entropy. Many image enhancement methods require selection of operating parameters, which are typically chosen using subjective methods, but these new measures allow for automated selection.We present experimental results for these methods and make a comparison against other leading algorithms.

## 11. Human-Visual-System-Inspired Underwater Image Quality Measures

Underwater images suffer from blurring effects, low contrast, and grayed out colors due to the absorption and scattering effects under the water. Many image enhancement algorithms for improving the visual quality of underwater images have been developed. Unfortunately, no well-accepted objective measure exists that can evaluate the quality of underwater images similar to human perception. Predominant underwater image processing algorithms use either a subjective evaluation, which is time consuming and biased, or a generic image quality measure, which fails to consider the properties of underwater images. To address this problem, a new nonreference underwater image
quality measure (UIQM) is presented in this paper. The UIQM comprises three underwater image attribute measures: the underwater image colorfulness measure (UICM), the underwater image
sharpness measure (UISM), and the underwater image contrast measure (UIConM). Each attribute is selected for evaluating one aspect of the underwater image degradation, and each presented
attribute measure is inspired by the properties of human visual systems (HVSs). The experimental results demonstrate that the measures effectively evaluate the underwater image quality in
accordance with the human perceptions. These measures are also used on the AirAsia 8501 wreckage images to show their importance in practical applications.

## 12. Image enhancement based on the statistics of visual representation

This paper introduces a novel algorithm to image enhancement that exploits the multi-scale wavelet and statistical characters of visual representation. Processing includes the global dynamic range (brightness) correction and local contrast adjustment, whose parameters are picked automatically by the information contained in the image itself. Experimental results show that the new algorithm outperforms other many existing image enhancement methods and is highly resilient to the effects of both the image-source variations.

## 13. Image inpainting via a control-theoretical model of human vision

In this paper we consider several algorithms for image inpainting based on the hypoelliptic diffusion naturally associated with a mathematical model of the primary visual cortex. In particular, we present one algorithm that does not exploit the information of where the image is corrupted, and others that do it. While the first algorithm is able to reconstruct only images that our visual system is still capable of recognize, we show that those of the second type completely transcend such limitation providing reconstructions at the state-of-the-art in image inpainting. This can be interpreted as a validation of the fact that our visual cortex actually encodes the first type of algorithm.

## 14. Infrared Image Saliency Detection based on Human Vision and Information Theory

Robust image saliency detection can process the image correctly without any prior knowledge and additional assumptions. Therefore, the saliency detection is still one of the important steps in the field of computer vision including object recognition and tracking, image and video encoding and image segmentation. Although infrared imaging has extensive applications, there is few saliency extraction algorithms based on infrared spectroscopy. We propose an infrared image-based saliency extraction algorithm based on human vision and information theory. The proposed algorithm uses both human visual attention mechanism and theory of information, and it can also produce a saliency image with full resolution. The detection results of the proposed algorithm get a higher accuracy and better recall rate, when tested on one of the largest infrared data sets which is publicly now and a data set created by ourselves.

## 15. INTEGRATING IMAGING AND VISION FOR CONTENT-SPECIFIC IMAGE ENHANCEMENT

The quality of real-world photographs can often be considerably improved by digital image processing .In this article we describe our approach, integrating imaging and vision, for content-specific image enhancement. According to our approach, the overall quality of digital photographs is improved by a modular, image enhancement procedure driven by the image content. Single processing modules can be considered as autonomous elements. The modules can be combined to improve the overall quality according to image and defect categories.

## 16. Interacting Linear and Nonlinear Characteristics Produce Population Coding Asymmetries between ON and OFF Cells in the Retina

The early visual system is a model for understanding the roles of cell populations in parallel processing. Cells in this system can be classified according to their responsiveness to different stimuli; a prominent example is the division between cells that respond to stimuli of opposite contrasts (ON vs OFF cells). These two cell classes display many asymmetries in their physiological characteristics (including temporal characteristics, spatial characteristics, and nonlinear characteristics) that, individually, are known to have important roles in population coding. Here we describe a novel distinction between the information that ON and OFF ganglion cell populations carry in mouse√¢‚Ç¨‚Äùthat OFF cells are able to signal motion information about both light and dark objects, while ON cells have a selective deficit at signaling the motion of dark objects. We found that none of the previously reported asymmetries in physiological characteristics could account for this distinction. We therefore analyzed its basis via a recently developed linear√¢‚Ç¨‚Äúnonlinear-Poisson model that faithfully captures input/output relationships for a broad range of stimuli (Bomash et al., 2013). While the coding differences between ON and OFF cell populations could not be ascribed to the linear or nonlinear components of the model individually, they had a simple explanation in the way that these components interact. Sensory transformations in other systems can likewise be described by these models, and thus our findings suggest that similar interactions between component properties may help account for the roles of cell classes in population coding more generally.

## 17. Natural color image enhancement and evaluation algorithm based on human visual system

To a significant degree, multimedia applications derive their effectiveness from the use of color graphics, images, and videos. In these applications, human visual system (HVS) often gives the final evaluation of the processed results. In this paper, we first propose a novel color image enhancement method, which is named HVS Controlled Color Image Enhancement and Evaluation algorithm (HCCIEE algorithm). We then applied the HCCIEE to color image by considering natural image quality metrics. This HCCIEE algorithm is base on multiscale representation of pattern, luminance, and color processing in the HVS. Experiments illustrated that the HCCIEE algorithm can produce distinguished details without ringing or halo artifacts. (These two problems often occur in conventional multiscale enhancement techniques.) As a result, the experimental results appear as similar as possible to the viewers√¢‚Ç¨‚Ñ¢ perception of the actual scenes.

## 18. Real-time Image Enhancement for Vision-based Autonomous Underwater Vehicle Navigation in MurkyWaters

Classic vision-based navigation solutions, which are utilized in algorithms such as Simultaneous Localization and Mapping (SLAM), usually fail to work underwater when the water is murky and the quality of the recorded images is low. That is because most SLAM algorithms are feature-based techniques and often it is impossible to extract the matched features from blurry underwater images. To get more useful features, image processing techniques can be used to dehaze the images before they are used in a navigation/ localization algorithm. There are many well-developed methods for image restoration, but the degree of enhancement and the resource cost of the methods are different. In this paper, we propose a new visual SLAM, specifically-designed for the underwater environment, using Generative Adversarial Networks (GANs) to enhance the quality of underwater images with underwater image quality evaluation metrics. This procedure increases the efficiency of SLAM and gets a better navigation and localization accuracy.We evaluate the proposed GANs-SLAM combination by using different images with various levels of turbidity in the water. Experiments were conducted and the data was extracted from the Carnegie Lake in Princeton, and the Raritan river both in New Jersey, USA.

## 19. Single Image Haze Removal from Image Enhancement Perspective for Real-Time Vision-Based Systems

Vision-based systems operating outdoors are significantly affected by weather conditions, notably those related to atmospheric turbidity. Accordingly, haze removal algorithms, actively being researched over the last decade, have come into use as a pre-processing step. Although numerous approaches have existed previously, an efficient method coupled with fast implementation is still in great demand. This paper proposes a single image haze removal algorithm with a corresponding hardware implementation for facilitating real-time processing. Contrary to methods that invert the physical model describing the formation of hazy images, the proposed approach mainly exploits computationally efficient image processing techniques such as detail enhancement, multiple-exposure image fusion, and adaptive tone remapping. Therefore, it possesses low computational complexity while achieving good performance compared to other state-of-the-art methods. Moreover, the low computational cost also brings about a compact hardware implementation capable of handling high-quality videos at an acceptable rate, that is, greater than 25 frames per second, as verified with a Field Programmable Gate Array chip. The software source code and datasets are available online for public use.

## 20. THE CONTRAST SENSITIVITY OF HUMAN COLOUR VISION TO RED-GREEN AND BLUE-YELLOW CHROMATIC GRATINGS

